# Prometheus Alert Rules
# Unity-Claude Automation Monitoring
# Version: 2025-08-24

groups:
  - name: system_alerts
    interval: 30s
    rules:
      # High CPU usage
      - alert: HighCPUUsage
        expr: 100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is above 80% (current value: {{ $value }}%)"

      # High memory usage
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is above 85% (current value: {{ $value }}%)"

      # Disk space low
      - alert: DiskSpaceLow
        expr: (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100 < 15
        for: 5m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Low disk space on {{ $labels.instance }}"
          description: "Disk space is below 15% (current value: {{ $value }}%)"

  - name: container_alerts
    interval: 30s
    rules:
      # Container down
      - alert: ContainerDown
        expr: up{job=~"langgraph-api|autogen-groupchat|powershell-modules"} == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Container {{ $labels.job }} is down"
          description: "Container {{ $labels.job }} has been down for more than 2 minutes"

      # Container restart
      - alert: ContainerRestarting
        expr: rate(container_restart_count[5m]) > 0
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "Container {{ $labels.name }} is restarting"
          description: "Container {{ $labels.name }} has restarted {{ $value }} times in the last 5 minutes"

      # High container CPU
      - alert: ContainerHighCPU
        expr: rate(container_cpu_usage_seconds_total[5m]) * 100 > 80
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} CPU usage is above 80% (current value: {{ $value }}%)"

      # High container memory
      - alert: ContainerHighMemory
        expr: container_memory_usage_bytes / container_spec_memory_limit_bytes * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage in container {{ $labels.name }}"
          description: "Container {{ $labels.name }} memory usage is above 85% (current value: {{ $value }}%)"

  - name: service_alerts
    interval: 30s
    rules:
      # LangGraph API health check failure
      - alert: LangGraphAPIUnhealthy
        expr: up{job="health-checks", service_name="langgraph-api"} == 0
        for: 2m
        labels:
          severity: critical
          team: ai-platform
          service: langgraph
        annotations:
          summary: "LangGraph API health check failing"
          description: "LangGraph API health check has been failing for more than 2 minutes"

      # AutoGen API health check failure
      - alert: AutoGenAPIUnhealthy
        expr: up{job="health-checks", service_name="autogen-groupchat"} == 0
        for: 2m
        labels:
          severity: critical
          team: ai-platform
          service: autogen
        annotations:
          summary: "AutoGen API health check failing"
          description: "AutoGen API health check has been failing for more than 2 minutes"

      # High API response time
      - alert: HighAPIResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
          team: ai-platform
        annotations:
          summary: "High API response time on {{ $labels.job }}"
          description: "95th percentile response time is above 2 seconds (current value: {{ $value }}s)"

      # High error rate
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
          team: ai-platform
        annotations:
          summary: "High error rate on {{ $labels.job }}"
          description: "Error rate is above 5% (current value: {{ $value | humanizePercentage }})"

  - name: monitoring_alerts
    interval: 30s
    rules:
      # Prometheus down
      - alert: PrometheusDown
        expr: up{job="prometheus"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Prometheus is down"
          description: "Prometheus has been down for more than 1 minute"

      # Grafana down
      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 2 minutes"

      # Loki down
      - alert: LokiDown
        expr: up{job="loki"} == 0
        for: 2m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Loki is down"
          description: "Loki log aggregation has been down for more than 2 minutes"

      # Alertmanager down
      - alert: AlertmanagerDown
        expr: up{job="alertmanager"} == 0
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "Alertmanager is down"
          description: "Alertmanager has been down for more than 1 minute"

      # Too many alerts firing
      - alert: TooManyAlerts
        expr: sum(ALERTS{alertstate="firing"}) > 10
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "Too many alerts are firing"
          description: "{{ $value }} alerts are currently firing, possible alert storm"